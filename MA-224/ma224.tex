\documentclass{article}

\usepackage[english]{babel}

\usepackage[letterpaper,top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{cancel}

\title{MA-224 Review}
\author{Jacob Sigman}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Probability}
\subsection{Basic Concepts}
\begin{description}
\item[Statistics] The study of collected data.
\item[Data] Qualitative or quantitative information that you obtain through experiments.
\item[Experiment] An unbiased, random, planned activity. Outcomes are simple, events are more complex.
\item[Frequency] The number of times that something occurs. There are three ways to display frequency: A frequency table, a bar graph, and a relative frequency table.
\item[Probability] Experimental and theoretical are two types. Theoretical is if an experiment is done many times. It is equivalent to \(\frac{\textnormal{\# of times event occurs}}{\textnormal{Total}}\).
\item[Probability Mass Function] A way of defining the relative frequency with a function.
\end{description}
\subsection{Properties of Probability}
A \textbf{set} is a group or collection of well-defined objects. The set of all possible outcomes is called the \textbf{sample space} and is denoted by \(\Omega\) or \(S\).
\subsubsection*{Operations of Sets}
\begin{description}
\item[Union] \(A \cup B=\{x\,\,|\,\,x\,\in\,A\,\,\&\,\,x\,\in\,B\}\)
\item[Intersection] \(A \cap B=\{x\,\,|\,\,x\,\in\,A\,\,\ \textnormal{or}\,\,x\,\in\,B\}\)
\item[Complement] \(\bar{A} \textnormal{ or } A^C=\{x\,\,|\,\,x\,\in\,A\,\,\&\,\,x\,\cancel{\in}\,A\}\)
\end{description}
\subsubsection*{Special Sets}
\begin{description}
\item[Universal Set] Set of all events, denoted by \(U\).
\item[Empty Set] Nothing in this set, denoted by \(\cancel{0}\).
\item[Singleton] A set with one element.
\item[Mutually Exclusive Sets] \(A \cap B = \cancel{0}\)
\item[Exhaustive Sets] Mutually exclusive but the union covers \(U\).
\item[Equally Likely Events] Events that have the same probability of happening.
\end{description}
\subsection{Methods of Enumeration}
\subsubsection*{Multiplication Principle}
If one experiment has \(n_1\) outcomes, and another experiment has \(n_2\) outcomes, the composite experiment will have \(n_1n_2\) outcomes.
\subsubsection*{Permutations}
Now suppose that that \(n\) positions are to be filled with \(n\) objects. There are \((n)(n-1)\hdots(2)(1)\) or \(n!\) possible arrangements. Each of these arrangements is called a \textbf{permutation}. A permutation of \(n\) objects with \(r\) taken at a time is denoted as follows.
\[_n\textnormal{P}_r=\frac{n!}{(n-r)!}\]
\subsubsection*{Combinations}
The number of ways in which \(r\) objects can be selected without replacement from \(n\) objects is called a \textbf{combination}. A combination of \(n\) objects with \(r\) taken at a time is denoted as follows.
\[\left(\begin{matrix}n\\r\end{matrix}\right)=_n\!\!\textnormal{C}_r=\frac{n!}{r!\,(n-r)!}\]
\subsection{Conditional Probability}
The \textbf{conditional probability} of an event, or the probability of \(A\) occurring given that \(B\) will occur is denoted as follows.
\[P(A\,|\,B)=\frac{P(A\,\cap\,B)}{P(B)}\]
By rearranging the above equation, the probability of both \(A\) and \(B\) occuring can be denoted as follows.
\[P(A\,\cap\,B)=P(A\,|\,B)*P(B)\]
\[P(A\,\cap\,B)=P(B\,|\,A)*P(A)\]
\subsection{Independent Events}
If the occurrence of one event will not change the probability of the occurrence of another event, these events are \textbf{independent}. Properties of independent events are as follows.
\[P(B\,|\,A)=P(B)\hspace{7 mm}P(A\,|\,B)=P(A)\]
\[P(A\,\cap\,B)=P(A)\,P(B)=P(B)\,P(A)\]
If an event does not satisfy these properties, the events are called \textbf{dependent}. Below are other sets that are independent if \(A\) and \(B\) are independent.
\begin{center}
    \(A\) and \(B'\)\hspace{7 mm}
    \(A'\) and \(B\)\hspace{7 mm}
    \(A'\) and \(B'\)
\end{center}
\subsection{Bayes's Theorem}
The probability of an event \(A\) can be calculated as follows.
\[P(A)=\sum_{i=1}^{m}P(B_i\,\cap\,A)=\sum_{i=1}^{m}P(B_i)\,P(A\,|\,B_i)\]
To determine the probability of a specific outcome \(B_k\), the equation for conditional probability can be used to obtain \textbf{Bayes's Theorem}, which is as follows.
\[P(B_k\,|\,A)=\frac{P(B_k)P(A\,|\,B_k)}{\sum\limits_{i=1}^{m}P(B_i)\,P(A\,|\,B_i)}\hspace{7 mm}\textnormal{for }k=1,2,3\hdots m\]
\section{Discrete Distributions}
\subsection{Random Variables of the Discrete Type}
Given a random experiment with outcome space \(S\), a function \(X\) that assigns one real number to each element int \(S\) is called a \textbf{random variable}. If \(S\) contains a finite number of points, or the points of \(S\) can be put into a one-to-one correspondence with positive integers, \(X\) becomes a random variable of the \textbf{discrete type}. A probability mass function of a discrete random variable satisfies the following properties.
\[f(x)>0\hspace{10mm}\sum\limits_{x\,\in\,S}f(x)=1\hspace{10 mm}P(X\,\in\,A)=\sum\limits_{x\,\in\,A}f(x)\]
There are two ways of representing a probability mass function. One way is with a \textbf{bar graph}, in which \(f(x)\) is represented with a vertical line segment. Another way  of representing a probability mass function is with a \textbf{probability histogram}, where each probability is depicted by a rectangle with base length 1.
\subsection{Mathematical Expectation}
There are many important characteristics of a discrete distribution. One is the \textbf{mathematical expectation}. The mathematical expectation of a function of \(X\) is as follows.
\[E[X]=\sum\limits_{x\,\in\,S}x\,f(x)\]
Below are some properties of the expected value of a function, \(E\).
\[E(c)=c\hspace{10 mm}E[cX]=cE[X]\]
\[E[c_1X+c_2X]=c_1E[X]+c_2E[X]\]
\subsection{The Mean, Variance, and Standard Deviation}
The \textbf{mean} of a random variable \(X\) is as follows.
\[\mu=\sum\limits_{x\in S}x\,f(x)=E[X]\]
The \textbf{variance} of the random variable \(X\), or of the distribution of \(X\) is as follows.
\[\sigma^2=\sum\limits_{x\in S}(x-\mu)^2\,f(x)=E([(X-\mu)]^2=E[X^2]-\mu^2\]
The \textbf{standard deviation} is the square root of the variance.
\[\sigma=\sqrt{\sigma^2}\]
The \(r^\textnormal{th}\) \textbf{moment} of the distribution about \(b\) is
\[E[(X-b)^r]=\sum\limits_{x\in S}(x-b)^rf(x)\]
The \(r^\textnormal{th}\) \textbf{factorial moment} is
\[E[(X)_r]=E[X(X-1)(X-2)\hdots (X-r+1)]\]
\subsection{Bernoulli Trials and the Binomial Distribution}
A \textbf{Bernoulli experiment} is a random experiment, the outcome of which can be classified as a success or a failure. A sequence of \textbf{Bernoulli trials} occurs when a Bernoulli experiment is performed several independent times so that the probability of success reminds the same from trial to trial. Let the probability of success be \(p\) and the probability of failure be \(q=1-p\).\\\\
If there are only outcomes of success and failure, we can say that \(X\) has a \textbf{Bernoulli distribution}. The expected value of \(X\) can be written as follows.
\[\mu=E[X]=\sum\limits_{x=0}^1xp^x(1-p)^{1-x}=p\]
The variance of \(X\) can be written as follows.
\[\sigma^2=\sum\limits_{x=0}^1(x-p)^2p^x(1-p)^{1-x}=\sqrt{pq}\]
If the trials are independent, and the probabilities of success and failure on each trial are \(p\) and \(q\) respectively, \(X\) has a \textbf{binomial distribution}, and the probability mass function of \(X\) is as follows.
\[f(x)=\left(\begin{matrix}n\\x\end{matrix}\right)p^x(1-p)^{n-x}\]
\subsection{The Moment-Generating Function}
Let \(X\) be a discrete random variable with probability mass function \(f(x)\) and space \(S\). If there is a positive number \(h\) such that \(-h \leq t \leq h\), the \textbf{Moment-Generating function} can be defined as follows.
\[E[e^{tX}]=\sum\limits_{x\in S}e^{tx}f(x)\]
The Moment-Generating function can be related to the mean and variance of a distribution.
\[\mu=M'(0)=E[X]\hspace{7 mm}\sigma^2=M''(0)-[M'(0)]^2=E[X^2]-E[X]^2\]
A \textbf{negative binomial distribution} has a probability mass function defined as follows.
\[f(x)=\left(\begin{matrix}x-1\\r-1\end{matrix}\right)\,p^r\,q^{x-r}\]
If a negative binomial distribution has an \(r\) value of 1, the distribution is said to be a \textbf{geometric distribution} which has a probability mass function as follows.
\[f(x)=p(1-p)^{x-1}\]
\subsection{The Poisson Distribution}
Let the number of changes that occur over a continuous interval be counted. We can then define an \textbf{approximate Poisson process} with a parameter \(\lambda>0\) if the following conditions are satified.
\begin{itemize}
    \item The number of changes occuring on nonoverlapping intervals are independent.
    \item The probability of exactly one change occuring in a sufficiently short interval of length \(h\) is about \(\lambda h\).
    \item The probability of two or more changes occuring in a sufficiently short interval is essentially zero.
\end{itemize}
We say that the random variable \(X\) has a \textbf{Poisson distribution} if its probability mass function is of the form
\[f(x)=\frac{\lambda^xe^{-\lambda}}{x!}\]
The mean (\(\mu\)) and variance (\(\sigma^2\)) of a Poisson distribution are both equal to \(\lambda\).
\section{Continuous Distributions}
\subsection{Continuous-Type Data}
Many experiments do not have integers as outcomes. If the measurements could come from an interval of possible outcomes, this data would be of the \textbf{continuous type}. For this type of data, group the data into classes and construct a histogram from these classes. The interval with the largest class height is called the \textbf{modal class} and the respective class mark is called the \textbf{mode}. There's more here but it wasn't really covered that much in class.
\subsection{Exploratory Data Analysis}
This really wasn't covered much either, but some notable things are the idea of \textbf{percentiles}. The 50th percentile of a distribution is known as the \textbf{median}, the 25th percentile of a distribution is known as the \textbf{first quartile}, and the 75th percentile of a distribution is known as the \textbf{third quartile}. The distance between the first and third quartile is called the \textbf{interquartile range (IQR)}. You can make something called a \textbf{box plot} summarizing the minimum, first quartile, median, third quartile, and maximum. This is called a \textbf{five-number summary}.
\subsection{Random Variables of the Continuous Type}
The relative frequency histogram \(h(x)\) associated with \(n\) observations of a random variable of the continuous type is a nonnegative function defined so that the total area between its graph and the x-axis equals 1. The probability over an interval given as \(P(a<X<b)\) is given as
\[\int_a^bf(x)\,dx\]
This means that the probability is the area bounded by the graph of \(f(x)\), the x-axis, and the lines \(x=a\) and \(x=b\). The \textbf{probability density function} of a random variable \(X\) of the \textbf{continuous type} with space \(S\) that is an interval or a union of integrals, is an integrable function of \(f(x)\) satisfying the following conditions.
\begin{itemize}
    \item \(f(x)>0\hspace{5 mm}x\in S\)
    \item \(\int_Sf(x)\,dx=1\)
    \item \(P(a<X<b)=\int_a^bf(x)\,dx\)
\end{itemize}
The \textbf{cumulative distribution function} of a random variable \(X\) of continuous type is as follows.
\[F(x)=P(X\leq x)=\int_{-\infty}^xf(t)\,dt\hspace{5 mm}-\infty <x<\infty\]
The mean, variance, and the moment-generating function of a continuous variable with a probability density function \(f(x)\).
\[\mu=E[X]=\int_{-\infty}^{\infty}x\,f(x)\,dx\]
\[\sigma^2=E[(X-\mu)^2]=\int_{-\infty}^{\infty}(x-\mu)^2\,f(x)\,dx\]
\[M(t)=\int_{-\infty}^{\infty}e^{tx}\,f(x)\,dx\]
\subsection{The Uniform and Exponential Distributions}
The random variable \(X\) has a \textbf{uniform distribution} if its probability density function is equal to a constant on its support. If the support is on the interval \([a,b]\), then
\[f(x)=\frac{1}{b-a}\hspace{5 mm}a \leq x \leq b\]
The mean, variance and moment-generating function of \(X\) are as follows.
\[\mu=\frac{a+b}{2}\hspace{8 mm}\sigma^2=\frac{(b-a)^2}{12}\hspace{8 mm}M(t)=\left\{\begin{matrix}\frac{e^{tb}-e^{ta}}{t(b-a)} & t \neq 0\\1 & t=0\end{matrix}\]
Sometimes in the Poisson process, we let \(\lambda=\frac{1}{\theta}\) and we say that random variable \(X\) has an \textbf{exponential distribution} if its probability density function is defined by
\[f(x)=\frac{1}{\theta}e^{\frac{-x}{\theta}}\]
The moment-generating function, mean, and variance of \(X\) are as follows.
\[M(t)=\int_0^{\infty}e^{tx}\left(\frac{1}{\theta}\right)e^{-\frac{x}{\theta}}\,dx\]
\[\mu=M'(0)=\theta\hspace{7 mm}\sigma^2=M''(0)-[M'(0)]^2=\theta^2\]
\subsection{The Gamma and Chi-Squared Distributions}
Chi-Squared distributions were not covered in class, so I'm only doing gamma. The \textbf{gamma function} is defined by
\[\Gamma(t)=\int_0^{\infty}y^{t-1}e^{-y}\,dy\]
The random variable \(X\) has a \textbf{gamma distribution} if its probability density function is defined by
\[f(x)=\frac{1}{\Gamma(\alpha)\theta^\alpha}\,x^{\alpha-1}e^{-\frac{x}{\theta}}\]
The moment-generating function, mean, and variance are
\[M(t)=\frac{1}{(1-\theta t)^\alpha}\hspace{7 mm}\mu=\alpha\theta\hspace{7 mm}\sigma^2=\alpha\theta^2=\frac{1}{1-\theta t}\]
\subsection{The Normal Distribution}
The random variable \(X\) has a \textbf{normal distribution} if it's probability density function is
\[f(x)=\frac{1}{\sigma\sqrt(2\pi)}\,\textnormal{exp}\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]\]
The moment-generating function, mean, and variance are as follows.
\[M(t)=\textnormal{exp}\left[\mu t + \frac{\sigma^2t^2}{2}\right]\]
\[\mu=E[X]=\mu\]
\[\sigma^2=M''(0)-[M'(0)]^2=\sigma^2\]
\end{document}
