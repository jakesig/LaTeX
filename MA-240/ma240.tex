\documentclass{article}

\usepackage[english]{babel}

\usepackage[letterpaper,top=2.5cm,bottom=2.5cm,left=2.5cm,right=2.5cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{bm}
\usepackage{booktabs, tabularx}
\usepackage[scr]{rsfso}

\newcommand{\lpl}{\mathscr{L}}

\title{MA-240 Review}
\author{Jacob Sigman}
\date{}

\begin{document}
\maketitle
\tableofcontents
\newpage
\part*{Chapter 1: Introduction to Differential Equations}
\addcontentsline{toc}{part}{Chapter 1: Introduction to Differential Equations}
\section*{1.1: Definitions and Terminology}
\addcontentsline{toc}{section}{1.1: Definitions and Terminology}
\begin{description}
    \item [Differential Equation] An equation containing the derivatives of one or more dependent variables with respect to one or more independent variables.
    \item [Classification by Order] The order of the highest derivative in the equation.
    \item [Classification by Type] Whether the differential equation has partial derivatives in it. If it does, it's called a \underline{Partial Differential Equation}, otherwise it's called an \underline{Ordinary Differential Equation}.
    \item [Classification By Linearity] A differential equation is linear if it can be put in the form \[a_n(x)\frac{d^ny}{dx^n}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+\cdots+a_0y=g(x)\] where \(a_i(x)\) and \(g(x)\) depend at most on the independent variable.
    \item [Solutions] Suppose \(\phi\) is a function defined on some interval. If substituting \(\phi\) into the ODE reduces the equation to an identity, then \(\phi\) solves the ODE on the interval. The graph of \(\phi\) is called a \underline{Solution Curve}.
    \item [Particular Solution] In general, an \(n^\textnormal{th}\) order Ordinary Differential Equation will be solved by an \(n\)-parameter family of solutions. A solution of a differential equation that is free of arbitrary parameters is said to be particular.
    \item [Singular Solution] A particular solution which does not belong to an \(n\)-parameter family of solutions.
    \item [General Solution] If the \(n\)-parameter family contains all the solutions, then the \(n\)-parameter family is said to be the general solution.
\end{description}
\section*{1.2: Initial-Value Problems}
\addcontentsline{toc}{section}{1.2: Initial-Value Problems}
An \underline{Initial-Value Problem} is an \(n^{\textnormal{th}}\) order Ordinary Differential Equation paired with conditions for the solution and it's first \((n-1)\) derivatives. Below is an example of an Initial-Value Problem.
\[y''+3y'-y=\sin x\hspace{7 mm}y(\pi)=1\hspace{7 mm}y'(\pi)=3\]
\underline{Picard's Theorem} discusses the existence of a unique solution. Suppose R is a region in the \(xy\)-plane containing \((x_0,y_0)\). If \(f(x,y)\) and \(\frac{\partial f}{\partial y}\) are continuous on R, there exists some interval I which belongs to \(x_0\) such that \(\frac{dy}{dx}=f(x,y)\) has one and only one solution passing through \(x_0\).
\section*{1.3: Differential Equations as Mathematical Models}
\addcontentsline{toc}{section}{1.3: Differential Equations as Mathematical Models}
\begin{description}
    \item [Population Dynamics] The rate at which population grows at a certain time is proportional to the total population of the country at that time: \(\frac{dP}{dt}=kP\)
    \item [Radioactive Decay] The rate at which the nuclei of a substance decays is proportional to the amount of substance remaining at a given time: \(\frac{dA}{dt}=kA\)
    \item [Newton's Law of Cooling] The rate at which the temperature of a body changes is proportional to the difference between the temperature of the body and the temperature of the surrounding medium: \(\frac{dT}{dt}=k(T-T_m)\)
    \item [Mixtures] The mixing of two salt solutions of differing concentrations results in a first-order differential equation for the amount of salt contained in a mixture: \(\frac{dA}{dt}=R_{in}-R_{out}\)
\end{description}
\part*{Chapter 2: First-Order Differential Equations}
\addcontentsline{toc}{part}{Chapter 2: First-Order Differential Equations}
\section*{2.1: Solution Curves Without a Solution}
\addcontentsline{toc}{section}{2.1: Solution Curves Without a Solution}
It's not always easy to solve a differential equation, we may just want to know what they look like. This is where \underline{Direction Fields} come in. They provide a visual representation of solutions. Evaluate some \(y'=f(x,y)\) on a dense grid.\\\\
An \underline{Autonomous} first-order differential equation is a differential equation which is only a function of \(y\): \(y'=f(x,y)=f(y)\). The points at which \(f(x,y)=0\) are called \underline{Critical Points}. A constant solution of an Autonomous differential equation is called an \underline{Equilibrium Solution}. A \underline{Phase Portrait} depicts the behavior of the differential equation on certain intervals. When two arrowheads of a phase portrait point towards eachother, it is said to be an \underline{attractor}, which is stable. When two arrowheads of a phase portrait point away from eachother, it is said to be a \underline{repeller}.
\section*{2.2: Seperable Equations}
\addcontentsline{toc}{section}{2.2: Seperable Equations}
A first-order equation of the following form is said to be \underline{separable}.
\[\frac{dy}{dx}=g(x)h(y)\]
This equation can be solved by integration.
\[\int p(y)y'dx=\int g(x)dx \hspace{2 mm}\rightarrow\hspace{2 mm}\int p(y)dy=\int g(x)dx\]
\section*{2.3: Linear Equations}
\addcontentsline{toc}{section}{2.3: Linear Equations}
Recall that a differential equation is linear if it can be put in the following form:
\[a_n(x)\frac{d^ny}{dx^n}+a_{n-1}(x)\frac{d^{n-1}y}{dx^{n-1}}+\cdots+a_0y=g(x)\,\,\textnormal{where}\,\,y'=P(x)y\]
We look for a function \(\mu\) by multiplying by \(\mu\) on both sides of \(y'\). We will get a derivative on one side.
\[\textnormal{Let }\mu(x)=e^{\int\!P(x)dx}\hspace{2 mm}\rightarrow\hspace{2 mm}\frac{d}{dx}\left[\mu(x)y\right]=f(x)e^{\int\!P(x)dx}\]
For a linear differential equation, if \(f(x)=0\) we call the differential equation \underline{homogeneous}, \(f(x)\) is sometimes called a \underline{forcing function}.\\\\
The error function and complimentary error function are defined as follows.
\[\textnormal{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^xe^{-t^2}dt\hspace{7 mm}\textnormal{erfc}(x)=\frac{2}{\sqrt{\pi}}\int_x^\infty e^{-t^2}dt\]
\section*{2.4: Exact Equations}
\addcontentsline{toc}{section}{2.4: Exact Equations}
Consider a differential equation of the following form:
\[\textbf{M}(x,y)dx+\textbf{N}(x,y)dy=0\hspace{3 mm}\rightarrow\hspace{3 mm}\textbf{M}(x,y)+\textbf{N}(x,y)y'=0\]
If \(z=f(x,y)\) is a 2-variable function with continuous partials in some region R, then on R, then on R, the \underline{total differential} of \(z\) is said to be:
\[dz=f_xdx+f_ydy\]
of particular interest to us is the case where \(f(x,y)=c\) so \(f_xdx+f_ydy=0\). \(f(x,y)=c\) is a family of functions defined by the parameter \(c\). \\\\
An expression \(\textbf{M}dx+\textbf{N}dy\) is an \underline{exact differential form} if there exists a function on a region R such that \(\textbf{M}=f_x\) and \(\textbf{N}=f_y\) are continuous. A differential equation is exact if:
\[\frac{\partial\textbf{M}}{\partial y}=\frac{\partial\textbf{N}}{\partial x}\]
If \(\textbf{M}_y\neq\textbf{N}_x\), then multiply both sides by a factor \(\mu\) such that \((\mu\textbf{M})_y=(\mu\textbf{N})_x\).
\[\mu_x+\left(\frac{\textbf{N}_x-\textbf{M}_y}{\textbf{N}}\right)\mu=0\hspace{3 mm}\textnormal{or}\hspace{3 mm}\mu_y+\left(\frac{\textbf{M}_y-\textbf{N}_x}{\textbf{M}}\right)\mu=0\]
\section*{2.5: Solutions by Substitutions}
\addcontentsline{toc}{section}{2.5: Solutions by Substitutions}
A function \(f\) is \underline{homogeneous} if \(f(tx,ty)=t^\alpha f(x,y)\) for all \(t,x,y\) of degree \(\alpha\). A differential equation \(\textbf{M}dx+\textbf{N}dy=0\) is homogeneous if \textbf{M} and \textbf{N} are homogeneous of the same degree. Substitutions for \(y\) and \(x\) are made as follows:
\[y=ux\hspace{3 mm}\textnormal{or}\hspace{3 mm}x=vy\]
\underline{Bernoulli's Equation} is as follows.
\[y'+P(x)y=f(x)y^n\hspace{5 mm}\textnormal{ Substitution: }u=y^{1-n}\]
\part*{Chapter 3: Modeling with First-Order Differential Equations}
\addcontentsline{toc}{part}{Chapter 3: Modeling with First-Order Differential Equations}
\section*{3.1: Linear Models}
\addcontentsline{toc}{section}{3.1: Linear Models}
\begin{center}
\begin{multicols}{3}
    \textbf{Growth and Decay}
    \[\frac{dx}{dt}=kx\]
    \textbf{Newton's Law of Cooling}
    \[\frac{dT}{dt}=k(T-T_m)\]
    \textbf{Mixtures}
    \[\frac{dA}{dt}=R_{in}-R_{out}\]
\end{multicols}
\vspace{5 mm}
\textbf{Series Circuits}
\[L\frac{di}{dt}+Ri=E(t)\hspace{7 mm}R\frac{dq}{dt}+\frac{1}{C}q=E(t)\]
\end{center}
\newpage
\part*{Chapter 4: Higher-Order Differential Equations}
\addcontentsline{toc}{part}{Chapter 4: Higher-Order Differential Equations}
\section*{4.1: Preliminary Theory - Linear Equations}
\addcontentsline{toc}{section}{4.1: Preliminary Theory - Linear Equations}
An \underline{\(n^{\textnormal{th}}\)-order Initial-Value Problem} is defined as
\[\sum_{i=0}^na_i(x)y^{(i)}(x)=g(x)\textnormal{ such that }y(x_0)=y_0, y'(x_0)=y_1,\cdots,y^{(n-1)}(x_0)=y_{(n-1)}\]
Let \(a_i\) be continuous on I. \(a_n(x)\neq 0\) for all \(x\in I\), then there is a unique solution to the \(n^{\textnormal{th}}\)-order Initial-Value Problem on I.
A \underline{boundary-value problem} is defined as
\[a_2(x)y''+a_1(x)y'+a_0(x)y=g(x)\textnormal{ such that }y(c)=y_0\textnormal{ and }y(b)=y_1\]
If \(g(x)=0\) in an \(n^{\textnormal{th}}\)-order Initial-Value Problem, the differential equation is homogeneous. Otherwise, it's not homogeneous. The \underline{differential operator} is defined as follows:
\[Df=f'\]
If \(\left\{y_i\right\}_{i=1}^k\) are solutions to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem, then \[\sum_{i=1}^kc_iy_i\]
is also a solution to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem. This is called the \underline{superposition principle}.\\\\
A set of functions \(\left\{f_i\right\}_{i=1}^n\) is \underline{linearly independent} if
\[\sum_{i=1}^nc_if_i(x)=0\]
This implies \(c_i=0\) for all \(i\) otherwise it's \underline{linearly dependent}. This means that there exists a set of constants \(\left\{c_i\right\}\) that are not all zero such that
\[\sum_{i=1}^nc_if_i(x)=0\].\\\\
Let \(F\) be a set of functions. If \(0\in F\), then \(F\) is linearly dependent. \(F=\left\{f_1,f_2\right\}\) is linearly dependent if and only if \(f_1=kf_2\). The \underline{Wronskian} of a set of functions \(\left\{f_i\right\}_{i=1}^n\) where \(f_i\) are differentiable up to \(n-1\) degrees is:
\[W(f_1,\cdots,f_n)=
\begin{vmatrix}
	f_1 & f_2 & \cdots & f_n \\
	f_1' & f_2' & \cdots & f_n' \\
	\vdots & \vdots & \ddots & \vdots \\
  f_1^{(n-1)} & f_2^{(n-1)} & \cdots & f_n^{(n-1)}
\end{vmatrix}
\]
A set of \(n\) solutions to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem is linearly independent if and only if the Wronskian is nonzero everywhere. If \(\left\{y_i\right\}_{i=1}^n\) are solutions to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem, then \(W(y_1,\cdots,y_n)\) is always or never zero on I. A set of linearly independent solutions to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem \(\left\{y_i\right\}_{i=1}^n\) is called a \underline{fundamental set}. There is always a fundamental set for the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem.\\\\
If \(\left\{y_i\right\}_{i=1}^n\) are solutions to the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem and \(y_p\) is a solution to the non-homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem then
\[\sum_{i=1}^nc_iy_i+y_p\]
is a solution to the non-homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem. If \(\left\{y_i\right\}_{i=1}^n\) are a fundamental set for the homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem and \(y_p\) solves the non-homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem, then the \underline{general solution} of the non-homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem is
\[y=\sum_{i=1}^nc_iy_i+y_p\]
To solve a non-homogeneous \(n^{\textnormal{th}}\)-order Initial-Value Problem, solve for the homogeneous case first, find the particular solution, then find the general solution. Say that \(y_{p_j}\) solves
\[\sum_{i=0}^na_i(x)y_i^{(i)}=g_j(x)\textnormal{ for }j=1,\cdots,m\]
\begin{center}then\end{center}
\[\sum_{j=1}^my_{p_j}\textnormal{ solves }\sum_{i=0}^na_i(x)y_i^{(i)}=\sum_{j=1}^mg_j(x)\]
\section*{4.2: Reduction of Order}
\addcontentsline{toc}{section}{4.2: Reduction of Order}
Say \(y_1\) is a known solution of
\[y''+P(x)y'+Q(x)y=f(x)\]
If \(y_2\) is another solution and \(\left\{y_1,y_2\right\}\) is a fundamental set, then we've found our solution. Use the following to find the second solution.
\[y_2(x)=u(x)y_1(x)\]
Below is the formula the textbook provides for reduction of order.
\[y_2(x)=y_1(x)\int\frac{e^{-\int P(x)dx}}{y_1(x)^2}dx\]
\section*{4.3: Homogeneous Linear Equations with Constant Coefficients}
\addcontentsline{toc}{section}{4.3: Homogeneous Linear Equations with Constant Coefficients}
The equation that is trying to be solved is
\[ay''+by'+cy=0\]
The \underline{characteristic equation} of the above equation can be written as follows
\[am^2+bm+c=0\]
There are three cases that can be analyzed based on the discriminant \((b^2-4ac)\) of the above characteristic equation. The first case is when the discriminant is greater than zero, and the characteristic equation has two distinct real roots \(m_1\) and \(m_2\).
\[y=c_1e^{m_1x}+c_2e^{m_2x}\]
The second case is when the discriminant is equal to zero, and the characteristic equation has a repeated root \(m\).
\[y=c_1e^{mx}+c_2xe^{mx}\]
The third case is when the discriminant is less than zero, and the characteristic equation has complex conjugate roots \(m_1=\alpha+i\beta\) and \(m_2=\alpha-i\beta\).
\[y=e^{\alpha x}(c_1\cos(\beta x))+c_2\sin(\beta x))\]
\section*{4.4: Undetermined Coefficients - Superposition Approach}
\addcontentsline{toc}{section}{4.4: Undetermined Coefficients - Superposition Approach}
The \underline{Method of Undetermined Coefficients} is one way of determining a particular solution. In solving differential equations, find the complementary solution, then find the particular solution, then put them both together using the superposition principle. \\\\
Guess a solution based on the non-homogeneous component of the differential equation and determine the coefficients of the guessed particular solution. Below are some examples of some guesses.\\\\
\def\arraystretch{1.5}%
\begin{tabularx}{\textwidth}{l X X r}
\toprule
\(\bm{g(x)}\) &&& \textbf{Form of} \(\bm{y_p}\) \\
\midrule
1 &&& \(A\) \\
\(5x+7\) &&& \(Ax+B\) \\
\(3x^2-2\) &&& \(Ax^2+Bx+C\) \\
\(\sin(4x)\) &&& \(A\cos(4x)+B\sin(4x)\) \\
\(e^{5x}\) &&& \(Ae^{5x}\) \\
\((9x-2)e^{5x}\) &&& \((Ax+B)e^{5x}\) \\
\((x^2)e^{5x}\) &&& \((Ax^2+Bx+C)e^{5x}\) \\
\(e^{3x}\sin(4x)\) &&& \(Ae^{3x}\cos(4x)+Be^{3x}\sin(4x)\) \\
\(xe^{3x}\sin(4x)\) &&& \((Ax+B)e^{3x}\cos(4x)+(Cx+D)e^{3x}\sin(4x)\) \\
\bottomrule
\end{tabularx}
\section*{4.6: Variation of Parameters}
\addcontentsline{toc}{section}{4.6: Variation of Parameters}
\underline{Variation of Parameters} is another approach to determining a particular solution. The formula below is used:
\[y_p=y_1\int\frac{y_1f(x)}{W(y_1,y_2)}dx+y_2\int\frac{y_2f(x)}{W(y_1,y_2)}dx\]
A more general form:
\[y_p=\sum_{i=1}^nu_iy_i\hspace{3 mm}\textnormal{where}\hspace{3 mm}u_i'=\frac{\Delta_i}{W(y_1,\cdots,y_n)}\]
\[\textnormal{Where } \Delta_i = W(y_1,\cdots,y_2) \text{ where the } i^\textnormal{th} \textnormal{ column is }
\begin{bmatrix}
0 \\
\vdots \\
f(x)
\end{bmatrix}
\]
\section*{4.7: Cauchy-Euler Equation}
\addcontentsline{toc}{section}{4.7: Cauchy-Euler Equation}
The \underline{Cauchy-Euler Equation} is a linear differential equation of the following form:
\[\sum_{i=0}^na_ix^iy^{(i)}(x)=g(x)\]
The characteristic equation for a Cauchy-Euler Equation is
\[am(m-1)+bm+c=0\]
There are three cases that can be analyzed based on the discriminant \((b^2-4ac)\) of the above characteristic equation. The first case is when the discriminant is greater than zero, and the characteristic equation has two distinct real roots \(m_1\) and \(m_2\).
\[y=c_1x^{m_1}+c_2x^{m_2}\]
The second case is when the discriminant is equal to zero, and the characteristic equation has a repeated root \(m\).
\[y=c_1x^{m}+c_2x^{m}\ln x\]
The third case is when the discriminant is less than zero, and the characteristic equation has complex conjugate roots \(m_1=\alpha+i\beta\) and \(m_2=\alpha-i\beta\).
\[y=x^{\alpha}(c_1\cos(\beta\ln x))+c_2\sin(\beta\ln x))\]
\part*{Chapter 5: Modeling with Higher-Order Differential Equations}
\addcontentsline{toc}{part}{Chapter 5: Modeling with Higher-Order Differential Equations}
\section*{5.1: Linear Models: Initial-Value Problems}
\addcontentsline{toc}{section}{5.1: Linear Models: Initial-Value Problems}
Linear models can describe \underline{simple harmonic motion}. The equation for the oscillator below is said to be \underline{undamped}.
\[\frac{d^2x}{dt^2}+\omega^2x=0\textnormal{ where }\omega^2=\frac{k}{m}\]
Sometimes, damping forces act on an oscillator. The equation for a \underline{damped} oscillator with damping constant \(\beta\) is
\[\frac{d^2x}{dt^2}+2\lambda\frac{dx}{dt}+\omega^2x=0\textnormal{ where }2\lambda=\frac{\beta}{m}\textnormal{ and }\omega^2=\frac{k}{m}\]
Sometimes, driving forces act on an oscillator. The equation for a \underline{driven} oscillator with driving force \(f(t)\) is
\[\frac{d^2x}{dt^2}+2\lambda\frac{dx}{dt}+\omega^2x=F(t)\textnormal{ where }2\lambda=\frac{\beta}{m}\textnormal{, }\omega^2=\frac{k}{m}\textnormal{, and }F(t)=\frac{f(t)}{m}\]
\part*{Chapter 6: Series Series Solutions of Linear Equations}
\addcontentsline{toc}{part}{Chapter 6: Series Series Solutions of Linear Equations}
\section*{6.1: Review of Power Series}
\addcontentsline{toc}{section}{6.1: Review of Power Series}
A \underline{power series} centered at \(a\) is a series in the form:
\[\sum_{n=0}^{\infty}c_n(x-a)^n\]
\begin{description}
    \item [Convergence] A power series is \underline{convergent} if it's sequence of partial sums converges. The limit of the partial sums is: \(\lim_{N\rightarrow\infty}\,\sum_{n=0}^Nc_n(x-a)^n\) If this limit doesn't exist, the power series is \underline{divergent}.
    \item [Interval of Convergence] The set of all real numbers \(x\) for which the series converges. The center of this interval is the center \(a\) of the series.
    \item [Radius of Convergence] The radius \(R\) of the interval of convergence. If the series only converges at it's center, \(R=0\), otherwise, a power series will converge for \(|x-a|<R\) and will diverge for \(|x-a|>R\).
    \item [Absolute Convergence] Within the interval of convergence, the power series converges absolutely.
\end{description}
One can test the convergence of a power series using the ratio test. Use the following limit:
\[\lim_{n\rightarrow\infty}\left|\frac{c_{n+1}(x-a)^{n+1}}{c_n(x-a)^n}\right|=L\]
If \(L<1\), the series converges absolutely, if \(L>1\) the series diverges, and if \(L=1\) the test is inconclusive.
\newline
\newline
The derivatives of power series are defined as follows.
\[y'=\sum_{n=1}^{\infty}c_nnx^{n-1}\hspace{7 mm}y''=\sum_{n=2}^{\infty}c_nn(n-1)x^{n-2}\]
If a power series is 0 for all numbers \(x\) in some open interval, then \(c_n=0\) for all \(n\). A function is \underline{analytic} at \(a\) if it has a power series representation at \(a\). Power series can also be combined through addition, multiplication, and division. You would do these as you would with polynomials. Below are some Maclaurin Series representations of common functions.\\\\
\def\arraystretch{1.5}%
\begin{tabularx}{\textwidth}{l X X r}
\toprule
\(\bm{f(x)}\) &&& \textbf{Maclaurin Series Representation} \\
\midrule
\(e^x\) &&& \(\sum\limits_{n=0}^\infty\frac{1}{n!}x^n\) \\
\(cos x\) &&& \(\sum\limits_{n=0}^\infty\frac{(-1)^n}{(2n)!}x^{2n}\) \\
\(\sin x\) &&& \(\sum\limits_{n=0}^\infty\frac{(-1)^n}{(2n+1)!}x^{2n+1}\) \\
\(\arctan x\) &&& \(\sum\limits_{n=0}^\infty\frac{(-1)^n}{2n+1}x^{2n+1}\) \\
\(\cosh x\) &&& \(\sum\limits_{n=0}^\infty\frac{1}{(2n)!}x^{2n}\) \\
\(\sinh x\) &&& \(\sum\limits_{n=0}^\infty\frac{1}{(2n+1)!}x^{2n+1}\) \\
\(\ln(1+x)\) &&& \(\sum\limits_{n=1}^\infty\frac{(-1)^{n+1}}{n}x^{n}\) \\
\(\frac{1}{1-x}\) &&& \(\sum\limits_{n=0}^\infty x^n\) \\
\bottomrule
\end{tabularx}
\section*{6.2: Solutions about Ordinary Points}
\addcontentsline{toc}{section}{6.2: Solutions about Ordinary Points}
A point \(x=x_0\) is said to be an \underline{ordinary point} of a differential equation if both \(P(x)\) and \(Q(x)\) are analytic at \(x_0\) in the differential equation below. A point that is not ordinary is said to be a \underline{singular point}.
\[y''+P(x)y'+Q(x)y=0\]
If \(x_0\) is an ordinary point of the above equation, then we can always find two linearly independent solutions of the above equation on an interval containing \(x_0\) of the form
\[y=\sum_{n=0}^\infty c_n(x-x_0)^n\textnormal{ on }|x-x_0|<R\]
Where the radius of convergence is the distance from \(x_0\) to the closest singular point. The general process is to combine all the series in a differential equation then to solve the recurrence relation once it's obtained.
\section*{6.3: Solutions about Singular Points}
\addcontentsline{toc}{section}{6.3: Solutions about Singular Points}
There are two types of singular points. A singular point is said to be a \underline{regular singular point} if \((x-x_0)P(x)\) and \((x-x_0)^2Q(x)\) are both analytic at \(x_0\). Otherwise, the point is said to be an \underline{irregular singular point}.
\newline
\newline
\underline{Frobenius's Theorem} states that if \(x=x_0\) is a regular singular point of the standard form of a differential equation then there exists at least one solution of the form
\[y=(x-x_0)^r\sum_{n=0}^\infty c_n(x-x_0)^n\]
Where \(r\) is some constant. After substituting the power series solution into the differential equation and simplifying, the \underline{indicial equation} is a quadratic equation in \(r\) that results from equating the total coefficient of the lowest power of \(x\) to zero. The values of \(r\), or the \underline{indicial roots} can be obtained and plugged into the recurrence relation.
\part*{Chapter 7: The Laplace Transform}
\addcontentsline{toc}{part}{Chapter 7: The Laplace Transform}
\section*{7.1: Definition of the Laplace Transform}
\addcontentsline{toc}{section}{7.1: Definition of the Laplace Transform}
Let \(k\) be a continuous function of two real variables, and \(g\) be a continuous function of one real variable. The \underline{integral transform} with \underline{kernel} \(k\) is defined as follows.
\[I_k(g)=\int_a^bk(s,t)g(t)dt=f(s)\]
Let \(f\) be a function defined for \(t\geq 0\). The \underline{Laplace transform} of \(f\) is
\[\lpl\{f(t)\}=\int_0^\infty e^{-st}f(t)dt\]
The Laplace transform is a \underline{linear transform}. A function \(f\) is said to be of \underline{exponential order} if there exist constants \(c\), \(M>0\), and \(T>0\) such that \(|f(t)|\leq Me^{ct}\) for all \(t>T\). If \(f\) is piecewise continuous on \([0, \infty)\) and of exponential order, then \(\lim_{s\rightarrow\infty}=0\). Below are some common Laplace transforms.
\newline
\newline
\def\arraystretch{1.5}%
\begin{tabularx}{\textwidth}{l X X r}
\toprule
\(\bm{f(t)}\) &&& \(\bm{\lpl\{s\}}\) \\
\midrule
\(1\) &&& \(\frac{1}{s}\) \\
\(t^n\) &&& \(\frac{n!}{s^{n+1}}\) \\
\(e^{at}\) &&& \(\frac{1}{s-a}\) \\
\(\sin kt\) &&& \(\frac{k}{s^2+k^2}\)\\
\(\cos kt\) &&& \(\frac{s}{s^2+k^2}\)\\
\(\sinh kt\) &&& \(\frac{k}{s^2-k^2}\)\\
\(\cosh kt\) &&& \(\frac{s}{s^2-k^2}\)\\
\bottomrule
\end{tabularx}
\newpage
\section*{7.2: Inverse Transforms and Transforms of Derivatives}
\addcontentsline{toc}{section}{7.2: Inverse Transforms and Transforms of Derivatives}
The Laplace transform can also be applied in reverse. Partial fractions play a key role in determining inverse Laplace transforms so that each term can be factored into distinct linear factors. Below are some common inverse Laplace transforms.
\newline
\newline
\def\arraystretch{1.5}%
\begin{tabularx}{\textwidth}{l X X r}
\toprule
\(\bm{\lpl\{s\}}\) &&& \(\bm{f(t)}\) \\
\midrule
\(\frac{1}{s}\) &&& \(1\) \\
\(\frac{n!}{s^{n+1}}\) &&& \(t^n\) \\
\(\frac{1}{s-a}\) &&& \(e^{at}\) \\
\(\frac{k}{s^2+k^2}\) &&& \(\sin kt\)\\
\(\frac{s}{s^2+k^2}\) &&& \(\cos kt\)\\
\(\frac{k}{s^2-k^2}\) &&& \(\sinh kt\)\\
\(\frac{s}{s^2-k^2}\) &&& \(\cosh kt\)\\
\bottomrule
\end{tabularx}
\newline
\newline
\newline
The transform of a derivative can be determined as well. If \(f, f',\,\hdots\,, f^{(n-1)}\) are continuous on \([0, \infty)\) and are of exponential order, and if \(f^{(n)}(t)\) is piecewise continuous on \([0,\infty)\), then
\[\lpl\{f^{(n)}(t)\}=s^nF(s)-s^{n-1}f(0)-s^{n-2}f'(0)-\cdots-f^{(n-1)}(0)\]
\section*{7.3: Operational Properties I}
\addcontentsline{toc}{section}{7.3: Operational Properties I}
It is possible to compute the Laplace transform of an exponential multiple of \(f\) by shifting the Laplace transform. If \(\lpl\{f(t)\}=F(s)\) and \(a\) is any real number, then the \underline{first shifting theorem} states that
\[\lpl\{e^{at}f(t)\}=F(s-a)\]
The \underline{unit step function} is defined as follows
\begin{equation*}
\mathscr{U}(t-a)=\mathscr{U}_a(t)=
    \begin{cases}
        0, &  0\leq t<a\\
        1, & t\geq a
    \end{cases}
\end{equation*}
Let \(F(s)=\lpl\{f(t)\}\). The properties of the unit step function with regard to Laplace transforms are as follows
\[\lpl\{\mathscr{U}_a(t)\}=\frac{e^{-sa}}{s}\hspace{7 mm}\lpl\{f(t-a)\,\mathscr{U}(t-a)\}=e^{-sa}F(s)\]
\section*{7.4: Operational Properties II}
\addcontentsline{toc}{section}{7.4: Operational Properties II}
The Laplace transform of the product of a function \(f(t)\) with \(t\) raised to a numerical exponent. This gives the derivative of a transform. If \(F(s)=\lpl\{f(t)\}\) then
\[\lpl\{t^nf(t)\}=(-1)^n\frac{d^n}{ds^n}F(s)\]
If functions \(f\) and \(g\) are piecewise continuous on the interval \([0,\infty)\), then the \underline{convolution} of \(f\) and \(g\) is defined by
\[f*g=\int_0^tf(\tau)\,g(t-\tau)d\tau\]
The \underline{convolution theorem} states that if \(f(t)\) and \(g(t)\) are piecewise continuous on \([0,\infty)\) and of exponential order, then
\[\lpl\{f*g\}=\lpl\{f(t)\}\,\lpl\{g(t)\}=F(s)\,G(s)\]
A \underline{Volterra integral equation} for \(f(t)\) is as follows
\[f(t)=g(t)+\int_0^tf(\tau)\,h(t-\tau)d\tau\]
A function is periodic with period \(T\) if \(f(t+T)=f(t)\) for all \(t\). If \(f(t)\) is piecewise continuous on \([0,\infty)\), of exponential order, and periodic with period \(T\), then
\[\lpl\{f(t)\}=\frac{1}{1-e^{-sT}}\int_0^Te^{-st}f(t)dt\]
\section*{7.5: The Dirac Delta Function}
\addcontentsline{toc}{section}{7.5: The Dirac Delta Function}
A function that is a \underline{unit impulse} is defined as follows
\begin{equation*}
\delta_a(t-t_0)=
    \begin{cases}
        0, &  0\leq t<t_0-a\\
        \frac{1}{2a}, & t_0-a\leq t < t_0+a\\
        0, & t\geq t_0+a
    \end{cases}
\end{equation*}
Typically it's convenient to work with another type of unit impulse called the \underline{Dirac delta function}, which can be characterized by two properties:
\begin{equation*}
\delta(t-t_0)=
    \begin{cases}
        \infty, &  t=t_0\\
        0, & t\neq t_0
    \end{cases}
    \hspace{10 mm}\int_0^\infty\delta(t-t_0)\,dt=1
\end{equation*}
The Laplace transform of the Dirac delta function is
\[\lpl\{\delta(t-t_0)\}=e^{-st_0}\]
\part*{Chapter 11: Fourier Series}
\addcontentsline{toc}{part}{Chapter 11: Fourier Series}
\section*{11.1: Orthogonal Functions}
\addcontentsline{toc}{section}{11.1: Orthogonal Functions}
An \underline{inner product} is a function \((\textbf{u},\textbf{v})\) such that
\begin{center}
\begin{multicols}{2}
    \((\textbf{u},\textbf{v})=(\textbf{v},\textbf{u})\)\\
    \((k\textbf{u},\textbf{v})=k(\textbf{u},\textbf{v})\)
    \((\textbf{u},\textbf{u})=0\textnormal{ if }\textbf{u}=0\textnormal{ and }(\textbf{u},\textbf{u})>0\textnormal{ if }u\neq 0\)
    \((\textbf{u}+\textbf{v},\textbf{w})=(\textbf{u},\textbf{w})+(\textbf{v},\textbf{w})\)
\end{multicols}
\end{center}
It's defined as the number
\[(f_1, f_2)=\int_a^bf_1(x)\,f_2(x)\,dx\]
The two functions \(f_1\) and \(f_2\) are \underline{orthogonal} if their inner product is 0. A set of real-valued functions \(\{\phi_i\}_{i=1}^n\) is said to be orthogonal if
\[(\phi_m,\phi_n)=\int_a^b\phi_m(x)\,\phi_n(x)\,dx=0\]
Additionally, if \((\phi_m, \phi_m)=1\), the set is said to be \underline{orthonormal}. The value \(\sqrt{(\phi_m,\phi_m)}\) denoted by \(||\phi_m||\) is called the \underline{norm} of \(\phi_m\) induced by the inner product. Any orthogonal set of nonzero functions can be made into an orthonormal set by \underline{normalizing} each function, dividing each function by its norm. It's also possible to represent funcitons in the form of a series. Given a function, when we can find \(\{\phi_n\}_{n=0}^\infty\) and \(\{c_n\}_{n=0}^\infty\) such that
\[f(x)=\sum_{n=0}^\infty c_n\phi_n(x)\]
If we can find \(\{\phi_n\}_{n=0}^\infty\), then
\[c_n=\frac{(f, \phi_n)}{||\phi_n||^2}\textnormal{ and }f(x)=\sum_{n=0}^\infty\frac{(f, \phi_n)}{||\phi_n||^2}\,\phi_n(x)\]
A set of real-valued functions is said to be \underline{orthogonal with respect to a weight function} on an interval if
\[\int_a^bw(x)\,\phi_m(x)\,\phi_n(x)\,dx=0\]
\section*{11.2: Fourier Series}
\addcontentsline{toc}{section}{11.2: Fourier Series}

\section*{11.3: Fourier Cosine and Sine Series}
\addcontentsline{toc}{section}{11.3: Fourier Cosine and Sine Series}
\part*{Chapter 12: Boundary-Value Problems in Rectangular Coordinates}
\addcontentsline{toc}{part}{Chapter 12: Boundary-Value Problems in Rectangular Coordinates}
\section*{12.1: Seperable Partial Differential Equations}
\addcontentsline{toc}{section}{12.1: Seperable Partial Differential Equations}
\section*{12.2: Classical PDEs and Boundary-Value Problems}
\addcontentsline{toc}{section}{12.2: Classical PDEs and Boundary-Value Problems}
\section*{12.3: Heat Equation}
\addcontentsline{toc}{section}{12.3: Heat Equation}
\section*{12.4: Wave Equation}
\addcontentsline{toc}{section}{12.4: Wave Equation}
\section*{12.5: Laplace's Equation}
\addcontentsline{toc}{section}{12.5: Laplace's Equation}
\end{document}
